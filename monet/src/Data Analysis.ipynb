{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense,BatchNormalization,Conv1D\n",
    "from keras.layers import Input,GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from utils import generate, generate_sim, OrnsteinUng\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,CSVLogger\n",
    "import datetime\n",
    "from keras.callbacks import History \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "\n",
    "batchsize = 32\n",
    "T = np.arange(19,21,0.1) # this provides another layer of stochasticity to make the network more robust\n",
    "steps = 20\n",
    " # number of steps to generate\n",
    "initializer = 'he_normal'\n",
    "f = 32 #number of filters\n",
    "sigma = 0 #noise variance\n",
    "\n",
    "inputs = Input((steps-1,1))\n",
    "\n",
    "x1 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=3,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=5,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,3,dilation_rate=3,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,3,dilation_rate=5,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "\n",
    "x5 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = GlobalMaxPooling1D()(x5)\n",
    "\n",
    "x7 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Conv1D(f,4,dilation_rate=3,padding='causal',activation='relu',kernel_initializer=initializer)(x7)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Conv1D(f,4,dilation_rate=5,padding='causal',activation='relu',kernel_initializer=initializer)(x7)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = GlobalMaxPooling1D()(x7)\n",
    "\n",
    "\n",
    "x6 = Conv1D(f,7,padding='same',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = GlobalMaxPooling1D()(x6)\n",
    "\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4,x5,x6])\n",
    "dense = Dense(512,activation='relu')(con)\n",
    "# dense = Dense(256,activation='relu')(dense)\n",
    "dense = Dense(128,activation='relu')(dense)\n",
    "# dense = Dense(64,activation='relu')(dense)\n",
    "# dense = Dense(32,activation='relu')(dense)\n",
    "dense2 = Dense(3,activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "         ReduceLROnPlateau(monitor='val_loss',\n",
    "                           factor=0.1,\n",
    "                           patience=4,\n",
    "                           verbose=1,\n",
    "                           min_lr=1e-9),\n",
    "         ModelCheckpoint(filepath=f'./Models/{steps}_model.h5',\n",
    "                         monitor='val_acc',\n",
    "                         save_best_only=False,\n",
    "                         mode='max',\n",
    "                         save_weights_only=False), history]\n",
    "\n",
    "\n",
    "gen = generate(batchsize=batchsize,steps=steps,T=T,sigma=sigma)\n",
    "model.fit_generator(generator=gen,\n",
    "        steps_per_epoch=50,\n",
    "        epochs=25,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=generate(batchsize=batchsize,steps=steps,T=T,sigma=sigma),\n",
    "        validation_steps=25)\n",
    "\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "train_acc  = history.history['acc']\n",
    "val_acc    = history.history['val_acc']\n",
    "xc         = range(25)\n",
    "\n",
    "\n",
    "##https://stackoverflow.com/questions/11026959/writing-a-dict-to-txt-file-and-reading-it-back\n",
    "with open(f'maxpoolingtwodense/{steps}_history.txt', 'wb') as handle:\n",
    "    pickle.dump(history.history, handle)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xc, train_loss, label=\"train loss\")\n",
    "plt.plot(xc, val_loss, label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"maxpoolingtwodense/{steps}_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sizes = []\n",
    "max_val_acc = []\n",
    "\n",
    "dir_path = os.getcwd() + \"added_layers\"\n",
    "matching_files = glob.glob( dir_path +\"\\*_history*\")\n",
    "\n",
    "for file in matching_files:\n",
    "    with open(file, 'rb') as handle:\n",
    "        history = pickle.loads(handle.read())\n",
    "        train_loss = history['loss']\n",
    "        val_loss   = history['val_loss']\n",
    "        train_acc  = history['acc']\n",
    "        val_acc    = history['val_acc']\n",
    "        \n",
    "        max_val_acc.append(max(val_acc))\n",
    "    string = [s for s in file if s.isdigit()]\n",
    "    actualstring = \"\"\n",
    "    for s in string:\n",
    "        actualstring += s\n",
    "    sizes.append(int(actualstring))\n",
    "zipped = zip(sizes, max_val_acc)\n",
    "sorted_pairs = sorted(zipped)\n",
    "sizes = [sorted_pairs[i][0] for i in range(len(sorted_pairs))]\n",
    "max_val_acc = [sorted_pairs[i][1] for i in range(len(sorted_pairs))]\n",
    "\n",
    "print(sorted_pairs)\n",
    "print(sizes)\n",
    "print(max_val_acc)\n",
    "plt.xlabel(\"number of training steps\")\n",
    "plt.ylabel(\"validation accuracy\")\n",
    "plt.title(\"Training steps vs max validation accuracy\")\n",
    "plt.plot(sizes, max_val_acc)\n",
    "\n",
    "#         plt.plot(xc, val_loss, label=\"validation loss\")\n",
    "#         plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_on_sim(net_file, num_steps):\n",
    "    dx,label=generate_sim(batchsize=100,steps=num_steps,T=15,sigma=0.00)\n",
    "    ### change here to load a different network model\n",
    "    print(dx)\n",
    "    N=np.shape(dx)[0]\n",
    "#     net_file = './Models/model_testsigma0-wB-6layern-300-variance-FBMCTRW.h5'\n",
    "    model = load_model(net_file)\n",
    "    predictions = []\n",
    "    for j in range(N):\n",
    "        dummy = np.zeros((1,num_steps-1,1))\n",
    "        dummy[0,:,:] = dx[j,:,:]\n",
    "        y_pred = model.predict(dummy) # get the results for 1D \n",
    "        print(y_pred)\n",
    "        ymean = np.mean(y_pred,axis=0) # calculate mean prediction of N-dimensional trajectory \n",
    "        prediction = np.argmax(ymean,axis=0) # translate to classification\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    accuracy = 100 * np.sum([1 if label[i] == predictions[i] else 0 for i in range(len(label))]) / len(label)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "# for i in range(10, 101, 10):\n",
    "accuracies.append(classification_on_sim('./Models/300_model.h5', 300))\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Number of steps trained on\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.title(\"Training steps vs Testing accuracy\")\n",
    "plt.plot(range(10, 101, 10),accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
